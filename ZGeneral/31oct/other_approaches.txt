Excellent â€” thatâ€™s the right mindset ğŸ‘
Youâ€™re already testing multiple query strategies (Approaches 0â€“3) to address LLM nondeterminism, which is the most practical way to stabilize responses. Since you also have ample compute and no time constraints, you can go further. Letâ€™s discuss systematic and advanced approaches you can add â€” from simple sampling to model-level control â€” so you can test and find the most reliable one.

ğŸ”¹ Root Cause Reminder

LLMs are nondeterministic because of:

Sampling randomness (temperature, top-p, etc.).

Latent ambiguity in the prompt itself.

Internal stochasticity in generation (even at temp=0, due to parallel decoding or GPU race conditions).

Hidden contextual variability (slight changes in whitespace, order, or hidden states can alter token paths).

âœ… Approach 4 â€“ Ensemble Averaging with Weighted Voting

Similar to your Approach 1, but use reason similarity weighting:

Run the same prompt 5â€“7 times.

Use cosine similarity (via embedding model like text-embedding-3-small) between all reasons.

Cluster similar answers and pick the cluster with maximum coherence.

Regenerate a concise explanation of the cluster centroid reasoning via LLM.

Why better: You capture both textual consistency and semantic alignment, not just majority of â€œYes/Noâ€.

âœ… Approach 5 â€“ Self-Consistency Decoding (borrowed from reasoning tasks)

Paper-inspired method (used in CoT reliability):

Generate n reasoning paths (say 10) with temperature=0.7.

Parse each to extract final â€œYes/Noâ€.

Choose the majority label.

Pass all the reasoning snippets to LLM again:
â€œSummarize the most consistent reasoning among the following justificationsâ€¦â€

Why: This method improves factual grounding and reduces hallucinations.
Code-wise: Itâ€™s almost like Approach-1 but collects and summarizes reasoning traces, not just votes.

âœ… Approach 6 â€“ Logit or Token-Level Determinism

If you can modify the model or use a local engine (since you mentioned Ollama and multiple GPUs):

Use temperature=0.0 and top-p=0.0 to force the greedy decoding path.

If the model still differs, set seed=constant in Ollama (check config: OLLAMA_SEED or via CLI --seed).

Some local engines allow beam search (beam width = 3â€“5) â†’ pick beam with most consistent â€œYes/Noâ€.

Why: Reduces randomness at the decoding layer itself.

âœ… Approach 7 â€“ Prompt Determinism Reinforcement

You can reframe the prompt to make answers more discrete:

â€œAnswer strictly with Yes or No followed by a short factual reason (max 50 words). Do not include uncertainty.
If ambiguous, choose the answer that is most supported by the text.â€

Also, add example pairs of clear Yes/No and reasoning.
LLMs become more stable when you define â€œdecision boundariesâ€ through few-shot patterns.

âœ… Approach 8 â€“ Cross-Model Agreement

If resources are not a problem:

Query 2 or 3 different models (e.g., mistral, llama3, gemma2).

Compare their outputs.

If 2/3 models agree â†’ accept that answer; otherwise â†’ regenerate through your main model.

Why: Cross-model ensemble improves robustness against single-model noise.

âœ… Approach 9 â€“ Consistency-Checking Post Filter

After getting the model output (say â€œYesâ€), verify it using a follow-up query:

â€œGiven this content and the answer â€˜Yesâ€™, is the reasoning logically consistent and supported by the text? Answer with a probability between 0 and 1.â€

If below threshold (e.g. <0.8), regenerate the response.

Why: This turns LLM into its own verifier, reducing spurious flips.

âœ… Approach 10 â€“ Persistent Memory Check

Keep a log of all 43 questions and their responses.
When re-running, instead of regenerating blindly:

Compare new answers to previous ones.

If mismatch â†’ ask model:

â€œPreviously, the answer was â€˜Noâ€™ with reason [...]. You now said â€˜Yesâ€™. Given the same content, explain which answer is more consistent.â€

This can stabilize future runs and reduce contradictions across sessions.

âœ… Suggested Plan for You
Approach	Description	Expected Stability	Cost	Notes
0	Single run	ğŸ”´ Low	ğŸŸ¢ Low	Baseline
1	5-vote majority	ğŸŸ¡ Medium	ğŸŸ¡ Medium	Simple ensemble
2	Yesâ†’Reason	ğŸŸ¢ Medium	ğŸŸ¡ Medium	Sequential reasoning
3	Reasonâ†’Yes	ğŸŸ¢ Medium	ğŸŸ¡ Medium	Opposite sequence
4	Weighted semantic vote	ğŸŸ¢ğŸŸ¢ High	ğŸ”´ High	Best for reliability
5	Self-consistency	ğŸŸ¢ğŸŸ¢ High	ğŸ”´ High	Great for reasoning tasks
6	Logit determinism	ğŸŸ¢ğŸŸ¢ğŸŸ¢ Very High	ğŸŸ¢ Low	If you control model
7	Structured prompt	ğŸŸ¢ğŸŸ¢ High	ğŸŸ¢ Low	Always combine with others
8	Cross-model	ğŸŸ¢ğŸŸ¢ğŸŸ¢ Very High	ğŸ”´ Very High	Needs multiple models
9	Verifier filter	ğŸŸ¢ğŸŸ¢ High	ğŸŸ¡ Medium	Good for contradiction removal
10	Memory-consistency	ğŸŸ¢ğŸŸ¢ğŸŸ¢ Very High	ğŸŸ¡ Medium	Long-term stability

Would you like me to show the full Python implementation for one or two of these advanced strategies â€” say Approach 4 (semantic voting) and Approach 5 (self-consistency decoding) â€” using your existing query_ollama setup?